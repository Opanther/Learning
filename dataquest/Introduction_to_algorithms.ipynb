{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Algorithm \n",
    "\n",
    "In this doc I will explain how to analyze how good an algorithm is in terms of speed and memory. Simply put, an algorithm is a program or function that solves some specific problem.  For example, a sorting algorithm is an algorithm that, given a list of values, outputs that same list of values but rearranges them in increasing (or decreasing) order.\n",
    "\n",
    "For any given problem, there are numerous ways to write an algorithm that solves it. In this course, you'll learn how to compare these algorithms and find the one that will perform the best. \n",
    "\n",
    "Let's start by writing an algorithm that, given a list of numbers, outputs the maximum value of that list. After all, before we can start analyzing algorithms, we need to have some algorithms to analyze.\n",
    "\n",
    "```python\n",
    "\n",
    "test_values = [4, 3, 5, 6, 2, 1]\n",
    "\n",
    "def maximum(values):\n",
    "    answer = None\n",
    "    for value in values:\n",
    "        if answer == none or answer < value:\n",
    "            answer = value \n",
    "    return answer\n",
    "\n",
    "max_value = maximum(test_values)\n",
    "```\n",
    "\n",
    "I'm going to learn ow to measure the execution time of a Python function. Ultimatley, the goal is not to measure the time of a specific execution of an algorithm, but rather to analyze the algorithm and predict how the execution time will evolve as data grows larger. \n",
    "\n",
    "Intuitively, the more data an algorithm needs to process, the more time it will take to run. What we are interested in is building a model that tells us by how much the execution time grows as we increase the amount of data. We call these models the **time complexity of an algorithm**. By analyzing the time complexity of an algorithm, we want to be able to answer questions like:\n",
    "\n",
    "*If we double the data, do we double the execution time, do we quadruple it, or something else entirely?*\n",
    "\n",
    "I can measure the execution time of a function to understand the runtime.\n",
    "\n",
    "Using list comprehensions, we can use the random.randint() function to generate a random list of length 500 with values, say, from -1,000 to 1,000, as follows:\n",
    "\n",
    "```python\n",
    "values = [random.randint(-1000, 1000) for _ in range(500)]\n",
    "\n",
    "```\n",
    "Notice that I used the _ notation in the above for loop. This is a notation that can be used when I do not use the iteration variable. It gives the exact same result that I would get using some variable name, but avoids having to find a name for something that I will not use.\n",
    "\n",
    "*Time complexity model* a mathematical expression for an algorithm, the expression gives me the ability to analyze an algorithm, it gives an insight on the rate at which it is increasing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "When calculating algorithm i dont want the exact execution time, I want to know how fast it is growing. Assume that each line of code takes some constant time to execute. \n",
    "```python \n",
    "def sum_values(values):\n",
    "    total = 0            # c1\n",
    "    for value in values: # c2\n",
    "        total += value   # c3\n",
    "    return total         # c4\n",
    "```\n",
    "\n",
    "The above comments mean that the first line takes some constant time c1 to be executed. The second takes c2 and so on. Then I can count how many times each line will be executed. This will depend on the length of the values list. denote this length by N and write the execution count of each line in front of it:\n",
    "\n",
    "```python\n",
    "def sum_values(values):\n",
    "    total = 0            # c1, 1 time\n",
    "    for value in values: # c2, N times\n",
    "        total += value   # c3, N times\n",
    "    return total         # c4, 1 time\n",
    "\n",
    "```\n",
    "\n",
    "Now multipy the execution time of each line by the number of times it is executed, and add those together: \n",
    "\n",
    "$$ C_{1} + C_{2} * N + C_{3} * N + C_{4} = (C_{2} + C_{3}) * N + (C_{1} + C_{4})$$\n",
    "\n",
    "The above equation can be further simplified by renaming $C_{2} + C_{3}$ as another constant **a** and $C_{1} + C_{4}$ as **b** we obtain a cleaner expression for the execution time in terms of the size of the input **(N)**:\n",
    "$$a * N + b $$ \n",
    "\n",
    "Regardless of the values of *a* and *b*, the function *aN + b* is a straight line. An algorithm whose time complexity is a straight line is called a **linear time algorithm**. These algorithmshave the property that the execution times grow proportionally to the data. \n",
    "\n",
    "When building a model for the exection time of an algorithm, we often focus on the worst case. \n",
    "\n",
    "1) We usually want to process data from a lot of different sources and, consequently, it turns out that the worst-case actually occurs quite often.\n",
    "\n",
    "2) It provides an upper bound. By focusing on the worst-case when building the execution time model, we can guarantee that the executions times will always behave at most as badly as the models predicts. Imagine that you are selling an algorithm that 1% of the time takes one second, and 99% takes over one year. If you advertise it as taking one second (best case), your customers will not be very pleased.\n",
    "\n",
    "**Constant time complexity** is an algorithm whose execution time does not depend on the data that it is processing. A constant time algorithm can be very slow; what characterizes it, is the fact that the time is indeendent of the amount of data. \n",
    "\n",
    "***\n",
    "\n",
    "Algorithm that help us to reason about finding an integer within a sorted list. This algorithm for searching a value in a sorted list is know as **binary search**. The name comes from the fact that at each iteration, we elimate half of the possible answesrs. \n",
    "\n",
    "```python\n",
    "def binary_search(values, target):                     \n",
    "    range_start = 0                                    # 1\n",
    "    range_end = len(values) - 1                        # 1\n",
    "    while range_start < range_end:                     # log(N)\n",
    "        range_middle = (range_end + range_start) // 2  # log(N)\n",
    "        value = values[range_middle]                   # log(N)\n",
    "        if value == target:                            # log(N)\n",
    "            return range_middle                        # log(N)\n",
    "        elif value < target:                           # log(N)\n",
    "            range_start = range_middle + 1             # log(N)\n",
    "        else:                                          # log(N)\n",
    "            range_end = range_middle - 1               # log(N)\n",
    "    if values[range_start] != target:                  # 1\n",
    "        return -1                                      # 1\n",
    "    return range_start                                 # 1\n",
    "\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}